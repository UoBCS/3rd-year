# Lecture 5 (05/10/2016)

## Memory management

Management of a **limited resource** need sophisticated algorithms and HW architectures to achieve efficient memory access by the CPU.
We divide the concept of addresses into 2:

- Logical address: program's view of the memory
- Physical address: actual HW memory addresses

To decrease the latency **caches** are added between the CPU and the main memory. We're also concerned about protection: the OS part of memory shouldn't be accessed
by user processes and the latters should not interfere directly with each other.

### Address binding

Classically, the binding of instructions and data to memory address can be done at any step along the way:

- Compile time: if you know at compile time where the process will reside in memory, then **absolute code** can be generated. The MS-DOS .COM-format programs are bount at compile time.

- Load time: if it is known at compile time where the process will reside in memory, then the compiler must generate **relocatable code**. In this case, final binding is delayed until load time.
If the starting address changes, we need only reload the user code to incorporate this changed value.

- Execution/Run time: special HW must be available. Most general-purpose OSes use this method.

### Logical vs Physical address space

An address generated by the CPU is commonly referred to as **logical address**, whereas an address seen by the memory unit is referred to as a **physical address**.
The compile and load time address binding methods generate identical logical and physical addresses.

In the case of run time address binding we refer at the logical address as **virtual address**. Thus, in the execution-time address-binding scheme, the logical and physical
address spaces differ.

The run-time mapping from virtual to physical addresses is done by a hardware device called the **MMU**.

![Dynamic relocation](https://github.com/UoBCS/3rd-year/blob/master/os/lecture-notes/assets/lecture5-mmu.png)

The user program never sees the real physical address.

### Dynamic loading

To obtain better memory-space utilization, we can use **dynamic loading**. With this technique, a routine is not loaded until it's called. All routines are kept on disk
in a relocatable load format.

The advantage of dynamic loading is that an unused routine is never loaded.

### Swapping

A process must be in memory to be executed. A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution.
It is usually combined with scheduling: if a higher-priority process arrives and wants service, the memory manager can swap out the lower-priority process and then load and execute
the higher-priority process.

Swapping requires a backing store. The backing store is commonly a fast
disk. It must be large enough to accommodate copies of all memory images
for all users, and it must provide direct access to these memory images. The
system maintains a ready queue consisting of all processes whose memory
images are on the backing store or in memory and are ready to run. Whenever
the CPU scheduler decides to execute a process, it calls the dispatcher. The
dispatcher checks to see whether the next process in the queue is in memory.
If it is not, and if there is no free memory region, the dispatcher swaps out a
process currently in memory and swaps in the desired process. It then reloads
registers and transfers control to the selected process.

Problem: the context-switch time in such a swapping system is fairly high.

### Contiguous memory allocation

The memory is usually divided into two partitions: one for the resident operating system and one for the user processes.
In contiguous memory allocation, each process is contained in a single contiguous section of memory.

One of the simplest methods for allocating memory is to divide memory into several fixed-sized **partitions**. Each partition may contain exactly one process.
Thus, the degree of multiprogramming is bound by the number of partitions.

In the scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied.
At any given time, then, we have a list of available block sizes and an input queue. The operating system can order the input queue according to a scheduling algorithm.

In generat as mentioned, the memory blocks available comprise a set of
holes of various sizes scattered throughout memory. When a process arrives
and needs memory, the system searches the set for a hole that is large enough
for this process. If the hole is too large, it is split into two parts. One part is
allocated to the arriving process; the other is returned to the set of holes.

If a new hole is adjacent to other holes, these adjacent holes are merged to form one larger hole.

The most common strategies to select which memory hole a process will use are:

- First fit: Allocate the first hole that is big enough.
- Best fit: Allocate the smallest hole that is big enough.
- Worst fit: Allocate the largest hole.

### Fragmentation

Both the first-fit and best-fit strategies for memory allocation suffer from **external fragmentation**.
External fragmentation exists
when there is enough total memory space to satisfy a request but the available
spaces are not contiguous; storage is fragmented into a large number of small holes.

Memory fragmentation can be internal as well as external (a solution is to divide the memory for processes into fixed size blocks).

One solution to the problem of external fragmentation is **compactation**. However it is possible only if relocation is dynamic and is done at execution time.

Another possible solution to the external-fragmentation problem is to
permit the logical address space of the processes to be noncontiguous, thus
allowing a process to be allocated physical memory wherever such memory
is available.

Two complementary techniques achieve this solution: **paging** and **segmentation** (which can be combined).

### Paging

![Paging](https://github.com/UoBCS/3rd-year/blob/master/os/lecture-notes/assets/lecture5-paging.png)

Traditionally, support for paging has been handled by hardware. However,
recent designs have implemented paging by closely integrating the hardware
and operating system, especially on 64-bit microprocessors.

The basic method for implementing paging involves breaking physical memory into fixed-sized blocks called **frames** and breaking logical memory into blocks of the same size called **pages**.
When a process is to be executed, its pages are loaded into any available memory frames from their source.

Every address generated by the CPU is divided into two parts: a **page number** and a **page offset**. The page number is used as an index into a **page table**.
The page table contains the base address of each page in physical memory. This base address is combined with the page offset to define the physical memory
address that is sent to the memory unit.

The size of a page is typically a power of 2. When we use a paging scheme, we have no external fragmentation: any free
frame can be allocated to a process that needs it. However, we may have some internal fragmentation.

An important aspect of paging is the clear separation between the user's
view of memory and the actual physical memory. The user program views
memory as one single space, containing only this one program. In fact, the user
program is scattered throughout physical memory, which also holds other
programs.

The **frame table** is has one entry for each physical page frame, indicating whether the latter if free or allocated and, if it is allocated, to which page of which process or processes.

### Segmentation

![Segmentation](https://github.com/UoBCS/3rd-year/blob/master/os/lecture-notes/assets/lecture5-segmentation.png)

Segmentation is a memory-management scheme that supports the user view of memory (i.e. collection of routines, data structures etc... referred to by a name).
The idea is to divide memory according to its usage.
A logical address space is a collection of segments. Each segment has a name and a length. Thus, a logical address consists of a two tuple: (segment number, offset).

### Virtual memory

All the above strategies have the same goal: to keep many
processes in memory simultaneously to allow multiprogramming. However,
they tend to require that an entire process be in memory before it can execute.

Virtual memory is a tecrucique that allows the execution of processes
that are not completely in memory. One major advantage of this scheme is
that programs can be larger than physical memory. Further, virtual memory
abstracts main memory into an extremely large, uniform array of storage,
separating logical memory as viewed by the user from physical memory.

Virtual memory involves the separation of logical memory as perceived
by users from physical memory. This separation allows an extremely large
virtual memory to be provided for programmers when only a smaller physical
memory is available.

### Demand paging

One option is to load the entire program in physical memory at program
execution time. However, a problent with this approach is that we may not
initially need the entire program in memory.

An alternative strategy is to load pages only as they are needed. This technique is known as **demand paging** and is commonly used in virtual memory systems.
With demand-paged virtual memory, pages are only loaded when they are
demanded during program execution; pages that are never accessed are thus
never loaded into physical memory.

There are 2 strategic decision to be made:

- Which process to **swap out** is done by the swapper.
- Which pages to move to disk when additional page is required: done by pager.

Minimisation of rate of page faults is very important.

#### Page replacement algorithms

1. FIFO: easy to implement but doesn't take **locality** into account. Another problem is that increasing the number of frames can increase the number of page faults (Belady's anomaly).
2. Optimal algorithm: select page which will be re-used at the latest time (or not at all). Not implementable because we need to look at the future
3. Least-recently used: use past as guide for future and replace page which has been unused for the longest time

#### Thrashing


